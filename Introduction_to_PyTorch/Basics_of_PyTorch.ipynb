{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a basic tensor, tensors are arrays used to represent and manipulate numerical data in deep learning. They serve as the foundational data structure for building and training neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor x: tensor([2, 1, 5, 5])\n",
      "Tensor y: tensor([6, 7, 6, 7])\n",
      "Sum of tensors: tensor([ 8,  8, 11, 12])\n",
      "Substraction of tensors: tensor([-4, -6, -1, -2])\n",
      "Multiplication of tensors: tensor([12,  7, 30, 35])\n"
     ]
    }
   ],
   "source": [
    "tensor_x = torch.tensor([random.randint(0, 10) for _ in range(4)])\n",
    "tensor_y = torch.tensor([random.randint(0, 10) for _ in range(4)])\n",
    "\n",
    "print(\"Tensor x:\",tensor_x)\n",
    "print(\"Tensor y:\",tensor_y)\n",
    "\n",
    "print(\"Sum of tensors:\", tensor_x + tensor_y)\n",
    "print(\"Substraction of tensors:\", tensor_x - tensor_y)\n",
    "print(\"Multiplication of tensors:\", tensor_x * tensor_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating basic neural network, in features have to be the same size as the length of the input features (original tensor and output of neurons). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7., 0., 2., 8., 9., 5.]])\n",
      "tensor([[-0.5542]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.Tensor([[random.randint(0, 10) for _ in range(6)]])\n",
    "print(input)\n",
    "\n",
    "#Small Neural Network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=6, out_features=3),  \n",
    "    nn.Linear(in_features=3, out_features=1)   \n",
    ")\n",
    "\n",
    "output = model(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid VS Softmax\n",
    "Sigmoid is an activation function that is mainly used in binary classification. Output bounday is usually 0.5. \n",
    "Softmax is an activation function that is mainly used in multi-classs scenarios. Predicted class is the one with highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7., 8., 4., 6., 7., 4., 8., 6.]])\n",
      "tensor([[0.5199]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input= torch.Tensor([[random.randint(0, 10) for _ in range(8)]])\n",
    "print(input)\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(in_features=8, out_features=1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  0.,  2.,  3., 10.,  6.,  3.,  5.,  8.,  1.,  4.]])\n",
      "tensor([[0.1151, 0.7037, 0.1034, 0.0778]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = torch.Tensor([[random.randint(0, 10) for _ in range(11)]])\n",
    "print(input)\n",
    "\n",
    "#Neural Network with 3 hidden neurons\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 10),\n",
    "  nn.Linear(10, 4),\n",
    "  nn.Softmax()\n",
    ")\n",
    "\n",
    "\n",
    "output = model(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function \n",
    "Function that determines how well your model is performing, comparing y_pred vs y. \n",
    "Uses One-hot encoding, transforming true label to a tensor of 1 and 0s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to reduce loss function\n",
    "\n",
    "- Backpropagation computes gradients of the loss with respect to the model parameters\n",
    "- Updating learning rate to find the global minimun\n",
    "- Updating parameters, like substracting local gradients by learning rate to the weights \n",
    "- PyTorch uses optimizers, takes cares of the weights, most common un stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.6885645389556885\n",
      "Loss: 1.6712253093719482\n",
      "Loss: 1.581771969795227\n",
      "Loss: 1.7591716051101685\n",
      "Loss: 1.6457157135009766\n",
      "Loss: 1.731720209121704\n",
      "Loss: 1.5870938301086426\n",
      "Loss: 1.6528093814849854\n",
      "Loss: 1.4827433824539185\n",
      "Loss: 1.7308361530303955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = torch.randn((100, 10)) \n",
    "y = torch.randint(0, 5, (100,)) #there are 5 classes\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 50),\n",
    "    nn.Linear(50, 5), #Output 5 features, as there are 5 classes\n",
    "    nn.Softmax() \n",
    ")\n",
    "\n",
    "# Loss function for multiclass classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "dataloader = DataLoader(dataset, shuffle=True)\n",
    "\n",
    "epochs = 10 #number of iterations\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in dataloader:\n",
    "        # Set the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Run a forward pass\n",
    "        feature, target = data\n",
    "        prediction = model(feature)    \n",
    "        \n",
    "        loss = criterion(prediction, target)    \n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Loss: {loss.item()}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fd5576acad3f7a858578a2aec3c3111a18ba7fcbb98d6518a6bbc5d9d4206a1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
