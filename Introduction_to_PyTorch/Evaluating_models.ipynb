{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juang\\AppData\\Local\\Temp\\ipykernel_5424\\1161177425.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "C:\\Users\\juang\\AppData\\Local\\Temp\\ipykernel_5424\\1161177425.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
      "C:\\Users\\juang\\AppData\\Local\\Temp\\ipykernel_5424\\1161177425.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val = torch.tensor(X_val, dtype=torch.float32)\n",
      "C:\\Users\\juang\\AppData\\Local\\Temp\\ipykernel_5424\\1161177425.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/water_potability.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "X = torch.tensor(df[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].values).float()\n",
    "y = torch.tensor(df['Potability'].values).float()\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data retrieved from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=12, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "print(model)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()  # binary cross-entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation Loop\n",
    "\n",
    "Looking at loss function and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.0866477489471436, Validation Loss: 0.8872309327125549\n",
      "Epoch 2/100, Training Loss: 0.6821507215499878, Validation Loss: 0.6924028992652893\n",
      "Epoch 3/100, Training Loss: 0.6694460511207581, Validation Loss: 0.6896106600761414\n",
      "Epoch 4/100, Training Loss: 0.6574300527572632, Validation Loss: 0.6872086524963379\n",
      "Epoch 5/100, Training Loss: 0.64174485206604, Validation Loss: 0.6854843497276306\n",
      "Epoch 6/100, Training Loss: 0.6270387768745422, Validation Loss: 0.683859646320343\n",
      "Epoch 7/100, Training Loss: 0.6111711859703064, Validation Loss: 0.6829855442047119\n",
      "Epoch 8/100, Training Loss: 0.5968443751335144, Validation Loss: 0.6822667121887207\n",
      "Epoch 9/100, Training Loss: 0.5842128992080688, Validation Loss: 0.6815661787986755\n",
      "Epoch 10/100, Training Loss: 0.5753598809242249, Validation Loss: 0.6815177798271179\n",
      "Epoch 11/100, Training Loss: 0.5686315894126892, Validation Loss: 0.6819702386856079\n",
      "Epoch 12/100, Training Loss: 0.5660653114318848, Validation Loss: 0.6821128726005554\n",
      "Epoch 13/100, Training Loss: 0.5673794150352478, Validation Loss: 0.6820241212844849\n",
      "Epoch 14/100, Training Loss: 0.5690817832946777, Validation Loss: 0.6822386980056763\n",
      "Epoch 15/100, Training Loss: 0.5697211027145386, Validation Loss: 0.6822755336761475\n",
      "Epoch 16/100, Training Loss: 0.5696362853050232, Validation Loss: 0.6824693083763123\n",
      "Epoch 17/100, Training Loss: 0.5696855187416077, Validation Loss: 0.6830494999885559\n",
      "Epoch 18/100, Training Loss: 0.5695763826370239, Validation Loss: 0.6834971308708191\n",
      "Epoch 19/100, Training Loss: 0.5682682991027832, Validation Loss: 0.6836243271827698\n",
      "Epoch 20/100, Training Loss: 0.5682620406150818, Validation Loss: 0.6834924221038818\n",
      "Epoch 21/100, Training Loss: 0.5685397982597351, Validation Loss: 0.6837620139122009\n",
      "Epoch 22/100, Training Loss: 0.5693436861038208, Validation Loss: 0.6834573149681091\n",
      "Epoch 23/100, Training Loss: 0.5710214972496033, Validation Loss: 0.6832288503646851\n",
      "Epoch 24/100, Training Loss: 0.5733540058135986, Validation Loss: 0.6829251646995544\n",
      "Epoch 25/100, Training Loss: 0.5736104846000671, Validation Loss: 0.6826220154762268\n",
      "Epoch 26/100, Training Loss: 0.5751569271087646, Validation Loss: 0.6823224425315857\n",
      "Epoch 27/100, Training Loss: 0.577333390712738, Validation Loss: 0.6819427609443665\n",
      "Epoch 28/100, Training Loss: 0.5784812569618225, Validation Loss: 0.6815029382705688\n",
      "Epoch 29/100, Training Loss: 0.580034613609314, Validation Loss: 0.6812494397163391\n",
      "Epoch 30/100, Training Loss: 0.5816880464553833, Validation Loss: 0.6808500289916992\n",
      "Epoch 31/100, Training Loss: 0.5861243605613708, Validation Loss: 0.6808754801750183\n",
      "Epoch 32/100, Training Loss: 0.5853070020675659, Validation Loss: 0.6805842518806458\n",
      "Epoch 33/100, Training Loss: 0.5853349566459656, Validation Loss: 0.6802594661712646\n",
      "Epoch 34/100, Training Loss: 0.5840350985527039, Validation Loss: 0.6806369423866272\n",
      "Epoch 35/100, Training Loss: 0.5789455771446228, Validation Loss: 0.6811739206314087\n",
      "Epoch 36/100, Training Loss: 0.5790947079658508, Validation Loss: 0.6810246109962463\n",
      "Epoch 37/100, Training Loss: 0.5780628323554993, Validation Loss: 0.6812427639961243\n",
      "Epoch 38/100, Training Loss: 0.5752323865890503, Validation Loss: 0.6823262572288513\n",
      "Epoch 39/100, Training Loss: 0.5734988451004028, Validation Loss: 0.683060348033905\n",
      "Epoch 40/100, Training Loss: 0.5696123242378235, Validation Loss: 0.6838135123252869\n",
      "Epoch 41/100, Training Loss: 0.568515419960022, Validation Loss: 0.6848217248916626\n",
      "Epoch 42/100, Training Loss: 0.5639209747314453, Validation Loss: 0.6870093941688538\n",
      "Epoch 43/100, Training Loss: 0.565211296081543, Validation Loss: 0.6868107318878174\n",
      "Epoch 44/100, Training Loss: 0.5623337626457214, Validation Loss: 0.6887304186820984\n",
      "Epoch 45/100, Training Loss: 0.5602231621742249, Validation Loss: 0.689554750919342\n",
      "Epoch 46/100, Training Loss: 0.5611386895179749, Validation Loss: 0.6895426511764526\n",
      "Epoch 47/100, Training Loss: 0.558781623840332, Validation Loss: 0.6915758848190308\n",
      "Epoch 48/100, Training Loss: 0.5580955147743225, Validation Loss: 0.6916896104812622\n",
      "Epoch 49/100, Training Loss: 0.559136688709259, Validation Loss: 0.691807746887207\n",
      "Epoch 50/100, Training Loss: 0.5608026385307312, Validation Loss: 0.691699206829071\n",
      "Epoch 51/100, Training Loss: 0.5605356097221375, Validation Loss: 0.6913731098175049\n",
      "Epoch 52/100, Training Loss: 0.5650551915168762, Validation Loss: 0.6903120279312134\n",
      "Epoch 53/100, Training Loss: 0.5666484832763672, Validation Loss: 0.6893170475959778\n",
      "Epoch 54/100, Training Loss: 0.5704966187477112, Validation Loss: 0.6888835430145264\n",
      "Epoch 55/100, Training Loss: 0.5790681838989258, Validation Loss: 0.6876368522644043\n",
      "Epoch 56/100, Training Loss: 0.6052311062812805, Validation Loss: 0.6844016909599304\n",
      "Epoch 57/100, Training Loss: 0.6035813689231873, Validation Loss: 0.684312105178833\n",
      "Epoch 58/100, Training Loss: 0.6045527458190918, Validation Loss: 0.6836905479431152\n",
      "Epoch 59/100, Training Loss: 0.6018856167793274, Validation Loss: 0.6834652423858643\n",
      "Epoch 60/100, Training Loss: 0.6037956476211548, Validation Loss: 0.6836127042770386\n",
      "Epoch 61/100, Training Loss: 0.6040436029434204, Validation Loss: 0.6837947368621826\n",
      "Epoch 62/100, Training Loss: 0.6046712398529053, Validation Loss: 0.6832102537155151\n",
      "Epoch 63/100, Training Loss: 0.6091875433921814, Validation Loss: 0.6822792291641235\n",
      "Epoch 64/100, Training Loss: 0.6051833033561707, Validation Loss: 0.6820036768913269\n",
      "Epoch 65/100, Training Loss: 0.6040854454040527, Validation Loss: 0.6804212927818298\n",
      "Epoch 66/100, Training Loss: 0.6014249920845032, Validation Loss: 0.6802049875259399\n",
      "Epoch 67/100, Training Loss: 0.6077660918235779, Validation Loss: 0.6815035343170166\n",
      "Epoch 68/100, Training Loss: 0.6031794548034668, Validation Loss: 0.679996907711029\n",
      "Epoch 69/100, Training Loss: 0.6035623550415039, Validation Loss: 0.6811420917510986\n",
      "Epoch 70/100, Training Loss: 0.6041257381439209, Validation Loss: 0.6807147264480591\n",
      "Epoch 71/100, Training Loss: 0.6025593280792236, Validation Loss: 0.6808678507804871\n",
      "Epoch 72/100, Training Loss: 0.6002278327941895, Validation Loss: 0.6799893379211426\n",
      "Epoch 73/100, Training Loss: 0.5985758900642395, Validation Loss: 0.6803936958312988\n",
      "Epoch 74/100, Training Loss: 0.6026357412338257, Validation Loss: 0.6801851987838745\n",
      "Epoch 75/100, Training Loss: 0.600716233253479, Validation Loss: 0.6803277730941772\n",
      "Epoch 76/100, Training Loss: 0.5999550819396973, Validation Loss: 0.6804440021514893\n",
      "Epoch 77/100, Training Loss: 0.6076325178146362, Validation Loss: 0.6803361773490906\n",
      "Epoch 78/100, Training Loss: 0.6086307764053345, Validation Loss: 0.6800509691238403\n",
      "Epoch 79/100, Training Loss: 0.605363130569458, Validation Loss: 0.6800212860107422\n",
      "Epoch 80/100, Training Loss: 0.6006454229354858, Validation Loss: 0.6807366013526917\n",
      "Epoch 81/100, Training Loss: 0.6044753193855286, Validation Loss: 0.6809132695198059\n",
      "Epoch 82/100, Training Loss: 0.6061680912971497, Validation Loss: 0.6805534362792969\n",
      "Epoch 83/100, Training Loss: 0.6046159863471985, Validation Loss: 0.6801681518554688\n",
      "Epoch 84/100, Training Loss: 0.6026256084442139, Validation Loss: 0.6804792284965515\n",
      "Epoch 85/100, Training Loss: 0.600666344165802, Validation Loss: 0.6809734106063843\n",
      "Epoch 86/100, Training Loss: 0.6041994094848633, Validation Loss: 0.6804758310317993\n",
      "Epoch 87/100, Training Loss: 0.6039119362831116, Validation Loss: 0.6805697083473206\n",
      "Epoch 88/100, Training Loss: 0.6051220297813416, Validation Loss: 0.6803656220436096\n",
      "Epoch 89/100, Training Loss: 0.604448676109314, Validation Loss: 0.6803457736968994\n",
      "Epoch 90/100, Training Loss: 0.6058620810508728, Validation Loss: 0.6798418760299683\n",
      "Epoch 91/100, Training Loss: 0.6070713996887207, Validation Loss: 0.6796888709068298\n",
      "Epoch 92/100, Training Loss: 0.6059184074401855, Validation Loss: 0.6799985766410828\n",
      "Epoch 93/100, Training Loss: 0.6047791242599487, Validation Loss: 0.6802819967269897\n",
      "Epoch 94/100, Training Loss: 0.6061416268348694, Validation Loss: 0.680250883102417\n",
      "Epoch 95/100, Training Loss: 0.6081656217575073, Validation Loss: 0.6801176071166992\n",
      "Epoch 96/100, Training Loss: 0.6072131395339966, Validation Loss: 0.6801719665527344\n",
      "Epoch 97/100, Training Loss: 0.6091173887252808, Validation Loss: 0.6801047325134277\n",
      "Epoch 98/100, Training Loss: 0.6094754338264465, Validation Loss: 0.679965615272522\n",
      "Epoch 99/100, Training Loss: 0.6069172620773315, Validation Loss: 0.6804179549217224\n",
      "Epoch 100/100, Training Loss: 0.6047044396400452, Validation Loss: 0.6807233095169067\n",
      "Training Accuracy 0.6026119589805603\n",
      "Validation Accuracy 0.5732010006904602\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training loop\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        Xbatch = X_train[i:i + batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y_train[i:i + batch_size]\n",
    "        loss = criterion(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(X_val)\n",
    "        val_loss = criterion(y_val_pred, y_val)\n",
    "    \n",
    "    # Print training and validation loss for each epoch\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}, Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "\n",
    "# Compute accuracy \n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_train)\n",
    "accuracy = (y_pred.round() == y_train).float().mean()\n",
    "print(f\"Training Accuracy {accuracy}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_val_pred = model(X_val)\n",
    "accuracy_val = (y_val_pred.round() == y_val).float().mean()\n",
    "print(f\"Validation Accuracy {accuracy_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "To reduce overfitting \n",
    "    - Reduce model size\n",
    "    - Using weights decay\n",
    "    - Obtain new data or augmentation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1fd5576acad3f7a858578a2aec3c3111a18ba7fcbb98d6518a6bbc5d9d4206a1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
